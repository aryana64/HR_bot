{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kazars24/customer-feedback-platform/"
      ],
      "metadata": {
        "id": "pXTos_Z8kthy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "1ee6a740-c62d-4d1b-b007-2530e35c0d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'customer-feedback-platform'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 78 (delta 24), reused 55 (delta 16), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (78/78), 52.99 KiB | 1.08 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python fire"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BQpM1i5w-elk",
        "outputId": "31059ef5-409f-4763-814e-b2d27706e633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.7.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.3.0)\n",
            "Building wheels for collected packages: llama-cpp-python, fire\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.7-cp310-cp310-manylinux_2_35_x86_64.whl size=6276580 sha256=5022f2c525f7813e83107b5b51dde504f31d954120899658bcd4a32077638fd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/74/f5/9f64ca4c6ff4c437f5566f52cfbc233660156f74edc3f1e5ec\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=3677aa68ab942582d833c13700b6e319022b513855ea60feaed76e0cc32ce7d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built llama-cpp-python fire\n",
            "Installing collected packages: fire, diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 fire-0.5.0 llama-cpp-python-0.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/IlyaGusev/saiga_7b_ggml/resolve/main/ggml-model-q4_1.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kxojOyhB_Z1F",
        "outputId": "e517fbe7-d012-4dee-a0d1-350183bc3480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-29 17:30:24--  https://huggingface.co/IlyaGusev/saiga_7b_ggml/resolve/main/ggml-model-q4_1.bin\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.180.30, 3.163.180.94, 3.163.180.117, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.180.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/1f/55/1f55a679d567c9ad9d58ea8a1eee1be3886262510c4dafc8f8b49fc403a2221a/388d91eb71cecb96dc0dbe813b39481e8e5e8411e0a9664531f33e2e19506578?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27ggml-model-q4_1.bin%3B+filename%3D%22ggml-model-q4_1.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1696267825&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NjI2NzgyNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8xZi81NS8xZjU1YTY3OWQ1NjdjOWFkOWQ1OGVhOGExZWVlMWJlMzg4NjI2MjUxMGM0ZGFmYzhmOGI0OWZjNDAzYTIyMjFhLzM4OGQ5MWViNzFjZWNiOTZkYzBkYmU4MTNiMzk0ODFlOGU1ZTg0MTFlMGE5NjY0NTMxZjMzZTJlMTk1MDY1Nzg%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=jpmC8F4E-f6gLj3OF%7Er1MR7yMbC7uo-U25dPhkqBmHguWUlUMvbPJvTZO2MN4QgbPxm1cSC3Nxqn7NUGqEQO-aX9SWkEXKZVS8e6BKGchykNl8E949iyLNBJhc28E3WBVqrwGk-EuraZi66vPufu0-%7Ea3wCf9dahoqAlRmhjX9zPxd8RcHAJbtHaarXX-AYgARDgNjCXjB%7EUvc6AimhelQwK9oS3IVrVSUgYuXMR6K%7Eg7TYuj0atrJJcsseo4ISrZLQME6dnXcfjtN61cA7fDcmFO49KZrAqVxWR1lH5Zl4SKz2bq0ivezFtun9AwCOV5x42hr1ukGGRC7YsEXrBSw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-09-29 17:30:25--  https://cdn-lfs.huggingface.co/repos/1f/55/1f55a679d567c9ad9d58ea8a1eee1be3886262510c4dafc8f8b49fc403a2221a/388d91eb71cecb96dc0dbe813b39481e8e5e8411e0a9664531f33e2e19506578?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27ggml-model-q4_1.bin%3B+filename%3D%22ggml-model-q4_1.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1696267825&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NjI2NzgyNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8xZi81NS8xZjU1YTY3OWQ1NjdjOWFkOWQ1OGVhOGExZWVlMWJlMzg4NjI2MjUxMGM0ZGFmYzhmOGI0OWZjNDAzYTIyMjFhLzM4OGQ5MWViNzFjZWNiOTZkYzBkYmU4MTNiMzk0ODFlOGU1ZTg0MTFlMGE5NjY0NTMxZjMzZTJlMTk1MDY1Nzg%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=jpmC8F4E-f6gLj3OF%7Er1MR7yMbC7uo-U25dPhkqBmHguWUlUMvbPJvTZO2MN4QgbPxm1cSC3Nxqn7NUGqEQO-aX9SWkEXKZVS8e6BKGchykNl8E949iyLNBJhc28E3WBVqrwGk-EuraZi66vPufu0-%7Ea3wCf9dahoqAlRmhjX9zPxd8RcHAJbtHaarXX-AYgARDgNjCXjB%7EUvc6AimhelQwK9oS3IVrVSUgYuXMR6K%7Eg7TYuj0atrJJcsseo4ISrZLQME6dnXcfjtN61cA7fDcmFO49KZrAqVxWR1lH5Zl4SKz2bq0ivezFtun9AwCOV5x42hr1ukGGRC7YsEXrBSw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.65.229.16, 18.65.229.83, 18.65.229.35, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.65.229.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4212859520 (3.9G) [application/octet-stream]\n",
            "Saving to: ‘ggml-model-q4_1.bin’\n",
            "\n",
            "ggml-model-q4_1.bin 100%[===================>]   3.92G  41.4MB/s    in 1m 44s  \n",
            "\n",
            "2023-09-29 17:32:09 (38.5 MB/s) - ‘ggml-model-q4_1.bin’ saved [4212859520/4212859520]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/IlyaGusev/saiga2_7b_gguf/resolve/main/model-q4_K.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aPwUedyaB3gt",
        "outputId": "e3ee4545-7de9-4852-b432-5905802cfab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-29 18:43:11--  https://huggingface.co/IlyaGusev/saiga2_7b_gguf/resolve/main/model-q4_K.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.88, 18.172.134.4, 18.172.134.124, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/5c/51/5c517a9e7f5c6a5b0428792a82e7a380cfeaabb4d93e66cbb82f023ed030f30d/b6d9ab608346c90e70eae6ab086524550c7ff5d5a34e6eb98b5b0531ef62b991?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-q4_K.gguf%3B+filename%3D%22model-q4_K.gguf%22%3B&Expires=1696272191&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NjI3MjE5MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy81Yy81MS81YzUxN2E5ZTdmNWM2YTViMDQyODc5MmE4MmU3YTM4MGNmZWFhYmI0ZDkzZTY2Y2JiODJmMDIzZWQwMzBmMzBkL2I2ZDlhYjYwODM0NmM5MGU3MGVhZTZhYjA4NjUyNDU1MGM3ZmY1ZDVhMzRlNmViOThiNWIwNTMxZWY2MmI5OTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=qwlMyV2U1JE2m8bJSCreTcPk9--VTK0FCg4CJeVsU5tZYbD9Cf0xtK8Mo%7EeRyOuR23sGGhaqj1UI9cvK0z%7E6nJXI5bq3q5usXz8RWLRErL65FtmWl8IB-PAh0Fs8KTs6UWr6kQwIiuVeXGEp8EkqTIh-PMmboKcNNNcZKQ0V-1FDquLI5jBCTBCHi5Do0VtvZHcDIlozFo9clxtWF9c9WbgR1AOkERhMtotKVN6oH73DNvSOlcOjoF5QVodgPMjMuWZaIVkJvvGGWh21QJdUivX0R11ACclhZnKLBq0TOEp6rxvcoLpORiP%7E4NuZrvs-a0ChXgIvGc8ob8SvjNZvKw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-09-29 18:43:11--  https://cdn-lfs.huggingface.co/repos/5c/51/5c517a9e7f5c6a5b0428792a82e7a380cfeaabb4d93e66cbb82f023ed030f30d/b6d9ab608346c90e70eae6ab086524550c7ff5d5a34e6eb98b5b0531ef62b991?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-q4_K.gguf%3B+filename%3D%22model-q4_K.gguf%22%3B&Expires=1696272191&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NjI3MjE5MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy81Yy81MS81YzUxN2E5ZTdmNWM2YTViMDQyODc5MmE4MmU3YTM4MGNmZWFhYmI0ZDkzZTY2Y2JiODJmMDIzZWQwMzBmMzBkL2I2ZDlhYjYwODM0NmM5MGU3MGVhZTZhYjA4NjUyNDU1MGM3ZmY1ZDVhMzRlNmViOThiNWIwNTMxZWY2MmI5OTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=qwlMyV2U1JE2m8bJSCreTcPk9--VTK0FCg4CJeVsU5tZYbD9Cf0xtK8Mo%7EeRyOuR23sGGhaqj1UI9cvK0z%7E6nJXI5bq3q5usXz8RWLRErL65FtmWl8IB-PAh0Fs8KTs6UWr6kQwIiuVeXGEp8EkqTIh-PMmboKcNNNcZKQ0V-1FDquLI5jBCTBCHi5Do0VtvZHcDIlozFo9clxtWF9c9WbgR1AOkERhMtotKVN6oH73DNvSOlcOjoF5QVodgPMjMuWZaIVkJvvGGWh21QJdUivX0R11ACclhZnKLBq0TOEp6rxvcoLpORiP%7E4NuZrvs-a0ChXgIvGc8ob8SvjNZvKw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.159.227.123, 108.159.227.71, 108.159.227.86, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.159.227.123|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4081004096 (3.8G) [binary/octet-stream]\n",
            "Saving to: ‘model-q4_K.gguf’\n",
            "\n",
            "model-q4_K.gguf     100%[===================>]   3.80G  35.0MB/s    in 88s     \n",
            "\n",
            "2023-09-29 18:44:39 (44.3 MB/s) - ‘model-q4_K.gguf’ saved [4081004096/4081004096]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/customer-feedback-platform\")"
      ],
      "metadata": {
        "id": "C9pxOi_f9mlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import src"
      ],
      "metadata": {
        "id": "JCuVs1tF-J9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.llm.agent import Agent"
      ],
      "metadata": {
        "id": "dFnKlSXZ-Lib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/IlyaGusev/rulm/master/self_instruct/src/interact_llamacpp.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dmVE-TAdg7q",
        "outputId": "9c7f5d25-8e4d-412f-f216-5a219ef2626c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-29 19:41:58--  https://raw.githubusercontent.com/IlyaGusev/rulm/master/self_instruct/src/interact_llamacpp.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1951 (1.9K) [text/plain]\n",
            "Saving to: ‘interact_llamacpp.py’\n",
            "\n",
            "interact_llamacpp.p 100%[===================>]   1.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-09-29 19:41:58 (30.4 MB/s) - ‘interact_llamacpp.py’ saved [1951/1951]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"/content/model-q4_K.gguf\"\n",
        "SYSTEM_PROMPT = \"Ты - русскоязычный ассистент. Тебе на вход поступает вопрос и контекст. Ответь на вопрос одним предложением и своими словами. Если ты не уверена в ответе, скажи, что не знаешь ответ.\""
      ],
      "metadata": {
        "id": "5SwGZpCP-ONz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(MODEL_PATH, SYSTEM_PROMPT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c-8s__HrBw-C",
        "outputId": "924a0061-158f-45e6-d3e4-ab30eed9eb02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = \"\"\"- Все встречи можно увидеть в Календаре.\n",
        "Есть разные типы встреч (о них более подробно можно почитать в нашем Кодексе Смартократии -прикрепила), и вкратце в кнопках\n",
        "- Тактическая встреча (ТВ) - здесь участники круга следят за достижениями Метрик, выполнении регулярных Проектов, разбирают и решают накопившиеся теншены, фиксируют действия и контролируют их исполнение.\n",
        "- Законодательная встреча (ЗВ) нужна для корректировки законодательства Круга (например, изменить предназначение и обязательства, избрать на роль и т.п.)\n",
        "- Целевые встречи проводятся для решения отдельных вопросов и могут включать в свой состав участников различных Кругов.\n",
        "- Agile встречи: планирование спринта, груминг, демо спринта, дейли, System Demo, участие в PI планировании, ретроспектива (эти страшные слова можно посмотреть в глоссарии)\n",
        "- Что на встрече? (Разберем саму встречу: повестка, «парковка», запись встречи, протокол встречи)\"\"\"\n",
        "QUESTION = \"Нужно самому мыть посуду?\""
      ],
      "metadata": {
        "id": "qssi5au-FBPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "answer = agent.interact(\"Текст:\" + TEXT + \"Вопрос:\" + QUESTION)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WfCx-qOkFh6-",
        "outputId": "4a6a29ed-6c73-41da-cb32-7a6541e7acda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 14.2 s, sys: 706 ms, total: 15 s\n",
            "Wall time: 15 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "vQ2rGk5GHo-B",
        "outputId": "89cd3be4-350b-454d-9ba2-08cb42ff0aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'6. Общие пространства - Наш офис\\n\\nУ нас есть несколько правил по нашим общим пространствам в офисе:\\n- На кухне у нас есть три холодильника:\\n    - два из них стандартно предназначены для хранения обедов сотрудников (постарайся не перепутать свой контейнер с контейнером коллеги ☺️)(номерки на картинке 1 и 2)\\n    - третий (со стеклянной дверцей) (3 на картинке, наш любименький) – общий, т.е. всё, что находится в нем может использовать любой сотрудник) Но, как известно, в большой семье... Так что успевай:-))\\n    - А что у нас с посудой? Мыть или не мыть, вот в чем вопрос\\n- Правила кухни и офиса:\\n  - Пролил что-то или просыпал - протри/подмети, пожалуйста (тряпка/веник и совок в сан.узле около кухни)\\n  - Увидел забытую грязную кружку/тарелку друга на кухонном столе - убери в посудомойку ;-)\\n  - Если брали вазу под цветы, после помой, пожалуйста, и протри ее))\\n  - Пожалуйста, пользуйся в офисе сменной обувью)Вопрос:Нужно самому мыть посуду?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import llama_cpp"
      ],
      "metadata": {
        "id": "MpPrPWHWLbov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama_cpp?"
      ],
      "metadata": {
        "id": "xj8Cw2DwLkn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain[all]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "NJ_t5c6mR0zB",
        "outputId": "ca8f0086-cefa-4b5d-aae8-f8adc7bd6512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain[all]\n",
            "  Downloading langchain-0.0.305-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain[all])\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain[all])\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.38 (from langchain[all])\n",
            "  Downloading langsmith-0.0.41-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (8.2.3)\n",
            "Collecting O365<3.0.0,>=2.0.26 (from langchain[all])\n",
            "  Downloading O365-2.0.31-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aleph-alpha-client<3.0.0,>=2.15.0 (from langchain[all])\n",
            "  Downloading aleph_alpha_client-2.17.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting amadeus>=8.1.0 (from langchain[all])\n",
            "  Downloading amadeus-9.0.0.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting arxiv<2.0,>=1.4 (from langchain[all])\n",
            "  Downloading arxiv-1.4.8-py3-none-any.whl (12 kB)\n",
            "Collecting atlassian-python-api<4.0.0,>=3.36.0 (from langchain[all])\n",
            "  Downloading atlassian_python_api-3.41.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.2/167.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting awadb<0.4.0,>=0.3.9 (from langchain[all])\n",
            "  Downloading awadb-0.3.10-cp310-cp310-manylinux1_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-ai-formrecognizer<4.0.0,>=3.2.1 (from langchain[all])\n",
            "  Downloading azure_ai_formrecognizer-3.3.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.8/297.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-ai-vision<0.12.0,>=0.11.1b1 (from langchain[all])\n",
            "  Downloading azure_ai_vision-0.11.1b1-py3-none-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-cognitiveservices-speech<2.0.0,>=1.28.0 (from langchain[all])\n",
            "  Downloading azure_cognitiveservices_speech-1.32.1-py3-none-manylinux1_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-cosmos<5.0.0,>=4.4.0b1 (from langchain[all])\n",
            "  Downloading azure_cosmos-4.5.1-py3-none-any.whl (230 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.6/230.6 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-identity<2.0.0,>=1.12.0 (from langchain[all])\n",
            "  Downloading azure_identity-1.14.0-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.2/160.2 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5,>=4 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (4.11.2)\n",
            "Collecting clarifai>=9.1.0 (from langchain[all])\n",
            "  Downloading clarifai-9.8.2-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting clickhouse-connect<0.6.0,>=0.5.14 (from langchain[all])\n",
            "  Downloading clickhouse_connect-0.5.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.7/922.7 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cohere<5,>=4 (from langchain[all])\n",
            "  Downloading cohere-4.27-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deeplake<4.0.0,>=3.6.8 (from langchain[all])\n",
            "  Downloading deeplake-3.7.2.tar.gz (554 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m554.7/554.7 kB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docarray[hnswlib]<0.33.0,>=0.32.0 (from langchain[all])\n",
            "  Downloading docarray-0.32.1-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting duckduckgo-search<4.0.0,>=3.8.3 (from langchain[all])\n",
            "  Downloading duckduckgo_search-3.8.6-py3-none-any.whl (18 kB)\n",
            "Collecting elasticsearch<9,>=8 (from langchain[all])\n",
            "  Downloading elasticsearch-8.10.0-py3-none-any.whl (414 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.0/415.0 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting esprima<5.0.0,>=4.0.1 (from langchain[all])\n",
            "  Downloading esprima-4.0.1.tar.gz (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss-cpu<2,>=1 (from langchain[all])\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-api-python-client==2.70.0 (from langchain[all])\n",
            "  Downloading google_api_python_client-2.70.0-py2.py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth<3.0.0,>=2.18.1 (from langchain[all])\n",
            "  Downloading google_auth-2.23.2-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.0/182.0 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-search-results<3,>=2 (from langchain[all])\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gptcache>=0.1.7 (from langchain[all])\n",
            "  Downloading gptcache-0.1.42-py3-none-any.whl (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.5/131.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting html2text<2021.0.0,>=2020.1.16 (from langchain[all])\n",
            "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
            "Collecting huggingface_hub<1,>=0 (from langchain[all])\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2<4,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (3.1.2)\n",
            "Collecting jq<2.0.0,>=1.4.1 (from langchain[all])\n",
            "  Downloading jq-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (656 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m656.0/656.0 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lancedb<0.2,>=0.1 (from langchain[all])\n",
            "  Downloading lancedb-0.1.16-py3-none-any.whl (34 kB)\n",
            "Collecting langkit<0.1.0,>=0.0.6 (from langchain[all])\n",
            "  Downloading langkit-0.0.19-py3-none-any.whl (769 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.1/769.1 kB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lark<2.0.0,>=1.1.5 (from langchain[all])\n",
            "  Downloading lark-1.1.7-py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting libdeeplake<0.0.61,>=0.0.60 (from langchain[all])\n",
            "  Downloading libdeeplake-0.0.60-cp310-cp310-manylinux2010_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: librosa<0.11.0,>=0.10.0.post2 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (0.10.1)\n",
            "Requirement already satisfied: lxml<5.0.0,>=4.9.2 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (4.9.3)\n",
            "Collecting manifest-ml<0.0.2,>=0.0.1 (from langchain[all])\n",
            "  Downloading manifest_ml-0.0.1-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marqo<2.0.0,>=1.2.4 (from langchain[all])\n",
            "  Downloading marqo-1.3.1-py3-none-any.whl (36 kB)\n",
            "Collecting momento<2.0.0,>=1.5.0 (from langchain[all])\n",
            "  Downloading momento-1.9.2-py3-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.4/134.4 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nebula3-python<4.0.0,>=3.4.0 (from langchain[all])\n",
            "  Downloading nebula3_python-3.4.0-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.4/312.4 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting neo4j<6.0.0,>=5.8.1 (from langchain[all])\n",
            "  Downloading neo4j-5.13.0.tar.gz (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.3/192.3 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx<4,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (3.1)\n",
            "Collecting nlpcloud<2,>=1 (from langchain[all])\n",
            "  Downloading nlpcloud-1.1.44-py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: nltk<4,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (3.8.1)\n",
            "Collecting nomic<2.0.0,>=1.0.43 (from langchain[all])\n",
            "  Downloading nomic-1.1.14.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openai<1,>=0 (from langchain[all])\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openlm<0.0.6,>=0.0.5 (from langchain[all])\n",
            "  Downloading openlm-0.0.5-py3-none-any.whl (10 kB)\n",
            "Collecting opensearch-py<3.0.0,>=2.0.0 (from langchain[all])\n",
            "  Downloading opensearch_py-2.3.1-py2.py3-none-any.whl (327 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.3/327.3 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer-six<20221106,>=20221105 (from langchain[all])\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m118.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pexpect<5.0.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (4.8.0)\n",
            "Collecting pgvector<0.2.0,>=0.1.6 (from langchain[all])\n",
            "  Downloading pgvector-0.1.8-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pinecone-client<3,>=2 (from langchain[all])\n",
            "  Downloading pinecone_client-2.2.4-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pinecone-text<0.5.0,>=0.4.2 (from langchain[all])\n",
            "  Downloading pinecone_text-0.4.2-py3-none-any.whl (17 kB)\n",
            "Collecting psycopg2-binary<3.0.0,>=2.9.5 (from langchain[all])\n",
            "  Downloading psycopg2_binary-2.9.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymongo<5.0.0,>=4.3.3 (from langchain[all])\n",
            "  Downloading pymongo-4.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m671.3/671.3 kB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyowm<4.0.0,>=3.3.0 (from langchain[all])\n",
            "  Downloading pyowm-3.3.0-py3-none-any.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf<4.0.0,>=3.4.0 (from langchain[all])\n",
            "  Downloading pypdf-3.16.2-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.3/276.3 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytesseract<0.4.0,>=0.3.10 (from langchain[all])\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Collecting python-arango<8.0.0,>=7.5.9 (from langchain[all])\n",
            "  Downloading python_arango-7.7.0-py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.4/108.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyvespa<0.34.0,>=0.33.0 (from langchain[all])\n",
            "  Downloading pyvespa-0.33.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting qdrant-client<2.0.0,>=1.3.1 (from langchain[all])\n",
            "  Downloading qdrant_client-1.5.4-py3-none-any.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdflib<7.0.0,>=6.3.2 (from langchain[all])\n",
            "  Downloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting redis<5,>=4 (from langchain[all])\n",
            "  Downloading redis-4.6.0-py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.1/241.1 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langchain[all])\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers<3,>=2 (from langchain[all])\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting singlestoredb<0.8.0,>=0.7.1 (from langchain[all])\n",
            "  Downloading singlestoredb-0.7.1-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.8/216.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-text<3.0.0,>=2.11.0 (from langchain[all])\n",
            "  Downloading tensorflow_text-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m116.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tigrisdb<2.0.0,>=1.0.0b6 (from langchain[all])\n",
            "  Downloading tigrisdb-1.0.0b6-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<0.6.0,>=0.3.2 (from langchain[all])\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (2.0.1+cu118)\n",
            "Collecting transformers<5,>=4 (from langchain[all])\n",
            "  Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting weaviate-client<4,>=3 (from langchain[all])\n",
            "  Downloading weaviate_client-3.24.1-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wikipedia<2,>=1 (from langchain[all])\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wolframalpha==5.0.0 (from langchain[all])\n",
            "  Downloading wolframalpha-5.0.0-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==2.70.0->langchain[all]) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==2.70.0->langchain[all]) (0.1.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==2.70.0->langchain[all]) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==2.70.0->langchain[all]) (4.1.1)\n",
            "Collecting xmltodict (from wolframalpha==5.0.0->langchain[all])\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from wolframalpha==5.0.0->langchain[all]) (10.1.0)\n",
            "Collecting jaraco.context (from wolframalpha==5.0.0->langchain[all])\n",
            "  Downloading jaraco.context-4.3.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (1.3.1)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all]) (2.0.4)\n",
            "Collecting aiodns>=3.0.0 (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\n",
            "Collecting aiohttp-retry>=2.8.3 (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading aiohttp_retry-2.8.3-py3-none-any.whl (9.8 kB)\n",
            "Collecting tokenizers>=0.13.2 (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all]) (4.5.0)\n",
            "Requirement already satisfied: Pillow>=9.2.0 in /usr/local/lib/python3.10/dist-packages (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all]) (9.4.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain[all]) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain[all]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain[all]) (1.1.3)\n",
            "Collecting feedparser (from arxiv<2.0,>=1.4->langchain[all])\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecated (from atlassian-python-api<4.0.0,>=3.36.0->langchain[all])\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from atlassian-python-api<4.0.0,>=3.36.0->langchain[all]) (1.16.0)\n",
            "Requirement already satisfied: oauthlib in /usr/local/lib/python3.10/dist-packages (from atlassian-python-api<4.0.0,>=3.36.0->langchain[all]) (3.2.2)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from atlassian-python-api<4.0.0,>=3.36.0->langchain[all]) (1.3.1)\n",
            "Collecting azure-core<2.0.0,>=1.23.0 (from azure-ai-formrecognizer<4.0.0,>=3.2.1->langchain[all])\n",
            "  Downloading azure_core-1.29.4-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msrest>=0.6.21 (from azure-ai-formrecognizer<4.0.0,>=3.2.1->langchain[all])\n",
            "  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-common~=1.1 (from azure-ai-formrecognizer<4.0.0,>=3.2.1->langchain[all])\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.10/dist-packages (from azure-identity<2.0.0,>=1.12.0->langchain[all]) (41.0.3)\n",
            "Collecting msal<2.0.0,>=1.20.0 (from azure-identity<2.0.0,>=1.12.0->langchain[all])\n",
            "  Downloading msal-1.24.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msal-extensions<2.0.0,>=0.3.0 (from azure-identity<2.0.0,>=1.12.0->langchain[all])\n",
            "  Downloading msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5,>=4->langchain[all]) (2.5)\n",
            "Collecting clarifai-grpc>=9.8.1 (from clarifai>=9.1.0->langchain[all])\n",
            "  Downloading clarifai_grpc-9.8.6-py3-none-any.whl (218 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.5/218.5 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tritonclient==2.34.0 (from clarifai>=9.1.0->langchain[all])\n",
            "  Downloading tritonclient-2.34.0-py3-none-manylinux1_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clarifai>=9.1.0->langchain[all]) (23.1)\n",
            "Collecting tqdm==4.64.1 (from clarifai>=9.1.0->langchain[all])\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich==13.4.2 (from clarifai>=9.1.0->langchain[all])\n",
            "  Downloading rich-13.4.2-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting schema==0.7.5 (from clarifai>=9.1.0->langchain[all])\n",
            "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich==13.4.2->clarifai>=9.1.0->langchain[all]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich==13.4.2->clarifai>=9.1.0->langchain[all]) (2.16.1)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema==0.7.5->clarifai>=9.1.0->langchain[all]) (21.6.0)\n",
            "Collecting python-rapidjson>=0.9.1 (from tritonclient==2.34.0->clarifai>=9.1.0->langchain[all])\n",
            "  Downloading python_rapidjson-1.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect<0.6.0,>=0.5.14->langchain[all]) (2023.7.22)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect<0.6.0,>=0.5.14->langchain[all]) (2023.3.post1)\n",
            "Collecting zstandard (from clickhouse-connect<0.6.0,>=0.5.14->langchain[all])\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from clickhouse-connect<0.6.0,>=0.5.14->langchain[all])\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff<3.0,>=2.0 (from cohere<5,>=4->langchain[all])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro==1.8.2 (from cohere<5,>=4->langchain[all])\n",
            "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere<5,>=4->langchain[all]) (6.8.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain[all])\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain[all])\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting boto3 (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading boto3-1.28.57-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake<4.0.0,>=3.6.8->langchain[all]) (8.1.7)\n",
            "Collecting pathos (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading pathos-0.3.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humbug>=0.3.1 (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading humbug-0.3.2-py3-none-any.whl (15 kB)\n",
            "Collecting numcodecs (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading numcodecs-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from deeplake<4.0.0,>=3.6.8->langchain[all]) (2.3.0)\n",
            "INFO: pip is looking at multiple versions of deeplake to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting deeplake<4.0.0,>=3.6.8 (from langchain[all])\n",
            "  Downloading deeplake-3.7.1.tar.gz (554 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m554.7/554.7 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aioboto3>=10.4.0 (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading aioboto3-11.3.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake<4.0.0,>=3.6.8->langchain[all]) (1.5.7)\n",
            "Collecting orjson>=3.8.2 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all])\n",
            "  Downloading orjson-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-requests>=2.28.11.6 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all])\n",
            "  Downloading types_requests-2.31.0.6-py3-none-any.whl (14 kB)\n",
            "Collecting hnswlib>=0.6.2 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all])\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all]) (3.20.3)\n",
            "Collecting aiofiles>=23.2.1 (from duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting httpx[brotli,http2,socks]>=0.25.0 (from duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting elastic-transport<9,>=8 (from elasticsearch<9,>=8->langchain[all])\n",
            "  Downloading elastic_transport-8.4.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0,>=2.18.1->langchain[all]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0,>=2.18.1->langchain[all]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0,>=2.18.1->langchain[all]) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1,>=0->langchain[all]) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1,>=0->langchain[all]) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4,>=3->langchain[all]) (2.1.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain[all])\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting pylance==0.5.10 (from lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading pylance-0.5.10-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ratelimiter (from lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading ratelimiter-1.2.0.post0-py3-none-any.whl (6.6 kB)\n",
            "Collecting retry (from lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting attr (from lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading attr-0.3.2-py2.py3-none-any.whl (3.3 kB)\n",
            "Collecting semver (from lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading semver-3.0.1-py3-none-any.whl (17 kB)\n",
            "Collecting pyarrow>=10 (from pylance==0.5.10->lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading pyarrow-13.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from langkit<0.1.0,>=0.0.6->langchain[all]) (1.5.3)\n",
            "Collecting textstat<0.8.0,>=0.7.3 (from langkit<0.1.0,>=0.0.6->langchain[all])\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting whylogs==1.2.6 (from langkit<0.1.0,>=0.0.6->langchain[all])\n",
            "  Downloading whylogs-1.2.6-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4.0.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from whylogs==1.2.6->langkit<0.1.0,>=0.0.6->langchain[all]) (3.10.0)\n",
            "Collecting whylabs-client<1,>=0.5.1 (from whylogs==1.2.6->langkit<0.1.0,>=0.0.6->langchain[all])\n",
            "  Downloading whylabs_client-0.5.7-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.0/402.0 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting whylogs-sketching>=3.4.1.dev3 (from whylogs==1.2.6->langkit<0.1.0,>=0.0.6->langchain[all])\n",
            "  Downloading whylogs_sketching-3.4.1.dev3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.3/547.3 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (1.11.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (1.7.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (0.3.6)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (1.0.5)\n",
            "Collecting dill>=0.3.5 (from manifest-ml<0.0.2,>=0.0.1->langchain[all])\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sqlitedict>=2.0.0 (from manifest-ml<0.0.2,>=0.0.1->langchain[all])\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.46.0 in /usr/local/lib/python3.10/dist-packages (from momento<2.0.0,>=1.5.0->langchain[all]) (1.57.0)\n",
            "Collecting momento-wire-types<0.76.0,>=0.75.0 (from momento<2.0.0,>=1.5.0->langchain[all])\n",
            "  Downloading momento_wire_types-0.75.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyjwt (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: future>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from nebula3-python<4.0.0,>=3.4.0->langchain[all]) (0.18.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4,>=3->langchain[all]) (2023.6.3)\n",
            "Collecting jsonlines (from nomic<2.0.0,>=1.0.43->langchain[all])\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting loguru (from nomic<2.0.0,>=1.0.43->langchain[all])\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wonderwords (from nomic<2.0.0,>=1.0.43->langchain[all])\n",
            "  Downloading wonderwords-2.2.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from O365<3.0.0,>=2.0.26->langchain[all]) (2.8.2)\n",
            "Collecting tzlocal<5.0,>=4.0 (from O365<3.0.0,>=2.0.26->langchain[all])\n",
            "  Downloading tzlocal-4.3.1-py3-none-any.whl (20 kB)\n",
            "Collecting stringcase>=1.2.0 (from O365<3.0.0,>=2.0.26->langchain[all])\n",
            "  Downloading stringcase-1.2.0.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting urllib3>=1.26 (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect<5.0.0,>=4.8.0->langchain[all]) (0.7.0)\n",
            "Collecting dnspython>=2.0.0 (from pinecone-client<3,>=2->langchain[all])\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mmh3<4.0.0,>=3.1.0 (from pinecone-text<0.5.0,>=0.4.2->langchain[all])\n",
            "  Downloading mmh3-3.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38 kB)\n",
            "Collecting torch<3,>=1 (from langchain[all])\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wget<4.0,>=3.2 (from pinecone-text<0.5.0,>=0.4.2->langchain[all])\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting geojson<3,>=2.3.0 (from pyowm<4.0.0,>=3.3.0->langchain[all])\n",
            "  Downloading geojson-2.5.0-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: PySocks<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from pyowm<4.0.0,>=3.3.0->langchain[all]) (1.7.1)\n",
            "Requirement already satisfied: setuptools>=42 in /usr/local/lib/python3.10/dist-packages (from python-arango<8.0.0,>=7.5.9->langchain[all]) (67.7.2)\n",
            "Collecting docker (from pyvespa<0.34.0,>=0.33.0->langchain[all])\n",
            "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.3.1->langchain[all])\n",
            "  Downloading grpcio_tools-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker<3.0.0,>=2.7.0 (from qdrant-client<2.0.0,>=1.3.1->langchain[all])\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting isodate<0.7.0,>=0.6.0 (from rdflib<7.0.0,>=6.3.2->langchain[all])\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib<7.0.0,>=6.3.2->langchain[all]) (3.1.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3,>=2->langchain[all]) (0.15.2+cu118)\n",
            "Collecting sentencepiece (from sentence-transformers<3,>=2->langchain[all])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build in /usr/local/lib/python3.10/dist-packages (from singlestoredb<0.8.0,>=0.7.1->langchain[all]) (1.0.3)\n",
            "Collecting sqlparams (from singlestoredb<0.8.0,>=0.7.1->langchain[all])\n",
            "  Downloading sqlparams-5.1.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from singlestoredb<0.8.0,>=0.7.1->langchain[all]) (0.41.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain[all]) (2.0.2)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (0.14.0)\n",
            "Requirement already satisfied: tensorflow<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (2.13.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<3,>=1->langchain[all])\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<3,>=1->langchain[all])\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<3,>=1->langchain[all])\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<3,>=1->langchain[all])\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5,>=4->langchain[all])\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting validators<1.0.0,>=0.21.2 (from weaviate-client<4,>=3->langchain[all])\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting authlib<2.0.0,>=1.2.1 (from weaviate-client<4,>=3->langchain[all])\n",
            "  Downloading Authlib-1.2.1-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiobotocore[boto3]==2.6.0 (from aioboto3>=10.4.0->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading aiobotocore-2.6.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.31.18,>=1.31.17 (from aiobotocore[boto3]==2.6.0->aioboto3>=10.4.0->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading botocore-1.31.17-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.6.0->aioboto3>=10.4.0->deeplake<4.0.0,>=3.6.8->langchain[all]) (1.15.0)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore[boto3]==2.6.0->aioboto3>=10.4.0->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting boto3 (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading boto3-1.28.17-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycares>=4.0.0 (from aiodns>=3.0.0->aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading pycares-4.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.7/288.7 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading s3transfer-0.6.2-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos>=1.53.0 in /usr/local/lib/python3.10/dist-packages (from clarifai-grpc>=9.8.1->clarifai>=9.1.0->langchain[all]) (1.60.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.12.0->langchain[all]) (1.15.1)\n",
            "Collecting protobuf>=3.19.0 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all])\n",
            "  Downloading protobuf-4.24.3-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio<2.0.0,>=1.46.0 (from momento<2.0.0,>=1.5.0->langchain[all])\n",
            "  Downloading grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.19.0,>=0.18.0 (from httpx[brotli,http2,socks]>=0.25.0->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2<5,>=3 (from httpx[brotli,http2,socks]>=0.25.0->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting socksio==1.* (from httpx[brotli,http2,socks]>=0.25.0->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading socksio-1.0.0-py3-none-any.whl (12 kB)\n",
            "Collecting brotli (from httpx[brotli,http2,socks]>=0.25.0->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere<5,>=4->langchain[all]) (3.16.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa<0.11.0,>=0.10.0.post2->langchain[all]) (0.39.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.18.1->langchain[all]) (0.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa<0.11.0,>=0.10.0.post2->langchain[all]) (3.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (2.3.0)\n",
            "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow<2.14,>=2.13.0 (from tensorflow-text<3.0.0,>=2.11.0->langchain[all])\n",
            "  Downloading tensorflow-2.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m479.7/479.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msrest>=0.6.21 (from azure-ai-formrecognizer<4.0.0,>=3.2.1->langchain[all])\n",
            "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-core<2.0.0,>=1.23.0 (from azure-ai-formrecognizer<4.0.0,>=3.2.1->langchain[all])\n",
            "  Downloading azure_core-1.29.3-py3-none-any.whl (191 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.4/191.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading azure_core-1.29.2-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.6/188.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading azure_core-1.29.1-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (0.33.0)\n",
            "Collecting pyphen (from textstat<0.8.0,>=0.7.3->langkit<0.1.0,>=0.0.6->langchain[all])\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-urllib3 (from types-requests>=2.28.11.6->docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all])\n",
            "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain[all])\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pytz-deprecation-shim (from tzlocal<5.0,>=4.0->O365<3.0.0,>=2.0.26->langchain[all])\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build->singlestoredb<0.8.0,>=0.7.1->langchain[all]) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build->singlestoredb<0.8.0,>=0.7.1->langchain[all]) (2.0.1)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyvespa<0.34.0,>=0.33.0->langchain[all]) (1.6.2)\n",
            "Collecting sgmllib3k (from feedparser->arxiv<2.0,>=1.4->langchain[all])\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from numcodecs->deeplake<4.0.0,>=3.6.8->langchain[all]) (0.4)\n",
            "Collecting ppft>=1.7.6.7 (from pathos->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading ppft-1.7.6.7-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.3 (from pathos->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading pox-0.3.3-py3-none-any.whl (29 kB)\n",
            "Collecting multiprocess>=0.70.15 (from pathos->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py<2.0.0,>=1.4.26 (from retry->lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision (from sentence-transformers<3,>=2->langchain[all])\n",
            "  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.12.0->langchain[all]) (2.21)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[brotli,http2,socks]>=0.25.0->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[brotli,http2,socks]>=0.25.0->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore<0.19.0,>=0.18.0->httpx[brotli,http2,socks]>=0.25.0->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich==13.4.2->clarifai>=9.1.0->langchain[all]) (0.1.2)\n",
            "INFO: pip is looking at multiple versions of pyjwt[crypto] to determine which version is compatible with other requirements. This could take a while.\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (2.3.7)\n",
            "Collecting tzdata (from pytz-deprecation-shim->tzlocal<5.0,>=4.0->O365<3.0.0,>=2.0.26->langchain[all])\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: amadeus, deeplake, esprima, google-search-results, neo4j, nomic, sentence-transformers, wikipedia, hnswlib, sqlitedict, stringcase, wget, sgmllib3k\n",
            "  Building wheel for amadeus (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for amadeus: filename=amadeus-9.0.0-py2.py3-none-any.whl size=75047 sha256=5b396aa3c18bd6461c44fac253fd0380c8b05513b2f01bf30a1dcbd4a7d37995\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/50/ea/3417d93eee6760a945d7711333d8d42b9f482e84600ef7f711\n",
            "  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplake: filename=deeplake-3.7.1-py3-none-any.whl size=669260 sha256=16a8d0cb98f45f3f75c2971060bf9e4a9e5e4e02dff1d222a718c0088827a740\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/5a/b2/c1da29595c6a68f8cc2816ad88f1a600e44a00f26f24c7e006\n",
            "  Building wheel for esprima (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for esprima: filename=esprima-4.0.1-py3-none-any.whl size=62240 sha256=372de55384dfa68100e1d576efa1a5859e1b3f665191039c6c5294b37096eab5\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/ad/8b/afd6e521e6aaea5482b7b4665ff3ce5a92373bd285e7d3a85c\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32003 sha256=a38110ddf8b2343de7bf0cd21cf72cc0494d06a871e59e6b90f2990a647232c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "  Building wheel for neo4j (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neo4j: filename=neo4j-5.13.0-py3-none-any.whl size=265313 sha256=8012b1df1f540ab3bdb411e14f083d6ad94c28799735c2e9af3889b6ff199c0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1d/b6/1be3a1e9de57bc832b7fcebbbf884186d8155bb6f1cc45be99\n",
            "  Building wheel for nomic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nomic: filename=nomic-1.1.14-py3-none-any.whl size=33821 sha256=646092ef8a6be0639a72220ddadb66433cc8e336e25e35b46ddabcf0b3e7e860\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/dd/9a/57a82068ce36ce73954949a52ebaa1745441728b8b0233421e\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=ea441a7523efc74c252ec46cd08fa474bd602d21aa7b2603f4e027a14a96d946\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=4f5c6ab7c898dc037f88dbcc61e5b3fe7f62aa40e7112578da274d28e14df0ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2202557 sha256=3e66535c7d8ff956a7f127f3db6b07d5df3440cb47f065b23722461e5aeb1f59\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=69580f6074960bb57854fc04e42eb6f6bd23dab822ecf5249060f7515876052c\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "  Building wheel for stringcase (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stringcase: filename=stringcase-1.2.0-py3-none-any.whl size=3568 sha256=aa968f587080b438ad13bb544fee39a0a342236394cc9f180b5227851438c87c\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/ba/22/1a2d952a9ce8aa86e42fda41e2c87fdaf20e238c88bf8df013\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=98f420d77fbd282b44ca76253082c1a61447d5c79ed0be1cb720e09c24534357\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=ae1b34d7ed559834835862cf73cd1b8a1294904d93072bfe7423ee6a6f2989aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built amadeus deeplake esprima google-search-results neo4j nomic sentence-transformers wikipedia hnswlib sqlitedict stringcase wget sgmllib3k\n",
            "Installing collected packages: whylogs-sketching, wget, types-urllib3, tokenizers, stringcase, sqlitedict, sgmllib3k, sentencepiece, safetensors, ratelimiter, mmh3, geojson, faiss-cpu, esprima, brotli, azure-common, attr, zstandard, xmltodict, wonderwords, validators, urllib3, tzdata, types-requests, tqdm, sqlparams, socksio, semver, schema, redis, python-rapidjson, pytesseract, pyphen, pypdf, pyjwt, pyarrow, py, psycopg2-binary, protobuf, ppft, pox, portalocker, pgvector, orjson, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, numcodecs, neo4j, mypy-extensions, marshmallow, lz4, loguru, libdeeplake, lark, jsonpointer, jsonlines, jq, jmespath, jaraco.context, isodate, hyperframe, html2text, hpack, hnswlib, h11, grpcio, feedparser, fastavro, dnspython, dill, deprecated, backoff, azure-cognitiveservices-speech, azure-ai-vision, awadb, amadeus, aioitertools, aiofiles, wolframalpha, whylabs-client, typing-inspect, tritonclient, textstat, rich, retry, rdflib, pytz-deprecation-shim, pymongo, pylance, pycares, nvidia-cudnn-cu11, nebula3-python, multiprocess, momento-wire-types, jsonpatch, httpcore, h2, grpcio-tools, google-auth, elastic-transport, clickhouse-connect, botocore, arxiv, wikipedia, whylogs, tzlocal, torch, tiktoken, tigrisdb, singlestoredb, s3transfer, requests-toolbelt, pinecone-client, pdfminer-six, pathos, opensearch-py, openlm, openai, nlpcloud, momento, marqo, manifest-ml, langsmith, lancedb, humbug, huggingface_hub, httpx, gptcache, google-search-results, elasticsearch, docker, docarray, dataclasses-json, cohere, clarifai-grpc, azure-core, authlib, aiohttp-retry, aiodns, aiobotocore, weaviate-client, transformers, torchvision, pyvespa, python-arango, pyowm, O365, nomic, msrest, msal, langkit, langchain, google-api-python-client, clarifai, boto3, azure-cosmos, atlassian-python-api, aleph-alpha-client, sentence-transformers, qdrant-client, msal-extensions, duckduckgo-search, azure-ai-formrecognizer, pinecone-text, azure-identity, aioboto3, tensorflow-text, deeplake\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: pyjwt\n",
            "    Found existing installation: PyJWT 2.3.0\n",
            "    Uninstalling PyJWT-2.3.0:\n",
            "      Successfully uninstalled PyJWT-2.3.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 9.0.0\n",
            "    Uninstalling pyarrow-9.0.0:\n",
            "      Successfully uninstalled pyarrow-9.0.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.57.0\n",
            "    Uninstalling grpcio-1.57.0:\n",
            "      Successfully uninstalled grpcio-1.57.0\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.5.2\n",
            "    Uninstalling rich-13.5.2:\n",
            "      Successfully uninstalled rich-13.5.2\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.17.3\n",
            "    Uninstalling google-auth-2.17.3:\n",
            "      Successfully uninstalled google-auth-2.17.3\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 5.0.1\n",
            "    Uninstalling tzlocal-5.0.1:\n",
            "      Successfully uninstalled tzlocal-5.0.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.84.0\n",
            "    Uninstalling google-api-python-client-2.84.0:\n",
            "      Successfully uninstalled google-api-python-client-2.84.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.17.3, but you have google-auth 2.23.2 which is incompatible.\n",
            "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 13.0.0 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.24.3 which is incompatible.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed O365-2.0.31 aioboto3-11.3.0 aiobotocore-2.6.0 aiodns-3.0.0 aiofiles-23.2.1 aiohttp-retry-2.8.3 aioitertools-0.11.0 aleph-alpha-client-2.17.0 amadeus-9.0.0 arxiv-1.4.8 atlassian-python-api-3.41.2 attr-0.3.2 authlib-1.2.1 awadb-0.3.10 azure-ai-formrecognizer-3.3.0 azure-ai-vision-0.11.1b1 azure-cognitiveservices-speech-1.32.1 azure-common-1.1.28 azure-core-1.29.1 azure-cosmos-4.5.1 azure-identity-1.14.0 backoff-2.2.1 boto3-1.28.17 botocore-1.31.17 brotli-1.1.0 clarifai-9.8.2 clarifai-grpc-9.8.6 clickhouse-connect-0.5.25 cohere-4.27 dataclasses-json-0.6.1 deeplake-3.7.1 deprecated-1.2.14 dill-0.3.7 dnspython-2.4.2 docarray-0.32.1 docker-6.1.3 duckduckgo-search-3.8.6 elastic-transport-8.4.1 elasticsearch-8.10.0 esprima-4.0.1 faiss-cpu-1.7.4 fastavro-1.8.2 feedparser-6.0.10 geojson-2.5.0 google-api-python-client-2.70.0 google-auth-2.23.2 google-search-results-2.4.2 gptcache-0.1.42 grpcio-1.59.0 grpcio-tools-1.59.0 h11-0.14.0 h2-4.1.0 hnswlib-0.7.0 hpack-4.0.0 html2text-2020.1.16 httpcore-0.18.0 httpx-0.25.0 huggingface_hub-0.17.3 humbug-0.3.2 hyperframe-6.0.1 isodate-0.6.1 jaraco.context-4.3.0 jmespath-1.0.1 jq-1.6.0 jsonlines-4.0.0 jsonpatch-1.33 jsonpointer-2.4 lancedb-0.1.16 langchain-0.0.305 langkit-0.0.19 langsmith-0.0.41 lark-1.1.7 libdeeplake-0.0.60 loguru-0.7.2 lz4-4.3.2 manifest-ml-0.0.1 marqo-1.3.1 marshmallow-3.20.1 mmh3-3.1.0 momento-1.9.2 momento-wire-types-0.75.0 msal-1.24.1 msal-extensions-1.0.0 msrest-0.7.1 multiprocess-0.70.15 mypy-extensions-1.0.0 nebula3-python-3.4.0 neo4j-5.13.0 nlpcloud-1.1.44 nomic-1.1.14 numcodecs-0.11.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 openai-0.28.1 openlm-0.0.5 opensearch-py-2.3.1 orjson-3.9.7 pathos-0.3.1 pdfminer-six-20221105 pgvector-0.1.8 pinecone-client-2.2.4 pinecone-text-0.4.2 portalocker-2.8.2 pox-0.3.3 ppft-1.7.6.7 protobuf-4.24.3 psycopg2-binary-2.9.8 py-1.11.0 pyarrow-13.0.0 pycares-4.3.0 pyjwt-2.8.0 pylance-0.5.10 pymongo-4.5.0 pyowm-3.3.0 pypdf-3.16.2 pyphen-0.14.0 pytesseract-0.3.10 python-arango-7.7.0 python-rapidjson-1.11 pytz-deprecation-shim-0.1.0.post0 pyvespa-0.33.0 qdrant-client-1.5.4 ratelimiter-1.2.0.post0 rdflib-6.3.2 redis-4.6.0 requests-toolbelt-1.0.0 retry-0.9.2 rich-13.4.2 s3transfer-0.6.2 safetensors-0.3.3 schema-0.7.5 semver-3.0.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 sgmllib3k-1.0.0 singlestoredb-0.7.1 socksio-1.0.0 sqlitedict-2.1.0 sqlparams-5.1.0 stringcase-1.2.0 tensorflow-text-2.13.0 textstat-0.7.3 tigrisdb-1.0.0b6 tiktoken-0.5.1 tokenizers-0.13.3 torch-1.13.1 torchvision-0.14.1 tqdm-4.64.1 transformers-4.33.3 tritonclient-2.34.0 types-requests-2.31.0.6 types-urllib3-1.26.25.14 typing-inspect-0.9.0 tzdata-2023.3 tzlocal-4.3.1 urllib3-1.26.16 validators-0.22.0 weaviate-client-3.24.1 wget-3.2 whylabs-client-0.5.7 whylogs-1.2.6 whylogs-sketching-3.4.1.dev3 wikipedia-1.4.0 wolframalpha-5.0.0 wonderwords-2.2.0 xmltodict-0.13.0 zstandard-0.21.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "llm = LlamaCpp(model_path='/content/model-q4_K.gguf')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0-pl5QRThtf",
        "outputId": "7829d600-89b2-48b4-c6ff-06f0a8d5ae08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = \"\"\"Ты — HR-ассистент. Ты отвечаешь на вопросы пользователей на основе корпоративной базы знаний.\n",
        "Если в базе знаний не указан ответ на вопрос, скажи, что не знаешь ответ.\n",
        "Отвечай коротко и четко. Всегда начинай диалог с фразы 'Добрый день!'. Корпоративная база знаний:\"\"\"\n",
        "prompt2 = \"\"\"Ты — HR-ассистент. Ты разговариваешь с людьми и помогаешь им. Отвечай вежливо, не забывай здороваться.\n",
        "Ты знаешь информацию из корпоративной базы знаний. Используй ее при ответе на вопросы. Если в базе знаний не содержится ответа на вопрос, скажи, что не знаешь отвта.\n",
        "Корпоративная база знаний:\"\"\""
      ],
      "metadata": {
        "id": "5E2K-puI1RQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Диалог"
      ],
      "metadata": {
        "id": "YMqzZXyCenGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"Ты — HR-ассистент. Твоя задача - помочь новому сотруднику разобраться в тонкостях корпоративной базы знаний.\n",
        "Отвечай на вопросы сотрудника, используя информацию из базы знаний. Отвечай подробно и простыми словами.\n",
        "Корпоративная база знаний:\"\"\"\n",
        "SYSTEM_TOKEN = 1788\n",
        "USER_TOKEN = 1404\n",
        "BOT_TOKEN = 9225\n",
        "LINEBREAK_TOKEN = 13\n",
        "\n",
        "ROLE_TOKENS = {\n",
        "    \"user\": USER_TOKEN,\n",
        "    \"bot\": BOT_TOKEN,\n",
        "    \"system\": SYSTEM_TOKEN\n",
        "}\n",
        "\n",
        "\n",
        "def get_message_tokens(model, role, content):\n",
        "    message_tokens = model.tokenize(content.encode(\"utf-8\"))\n",
        "    message_tokens.insert(1, ROLE_TOKENS[role])\n",
        "    message_tokens.insert(2, LINEBREAK_TOKEN)\n",
        "    message_tokens.append(model.token_eos())\n",
        "    return message_tokens\n",
        "\n",
        "\n",
        "def get_system_tokens(model, context):\n",
        "    system_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": SYSTEM_PROMPT + context\n",
        "    }\n",
        "    return get_message_tokens(model, **system_message)\n",
        "\n",
        "\n",
        "def interact(\n",
        "    model_path,\n",
        "    context,\n",
        "    n_ctx=2000,\n",
        "    top_k=30,\n",
        "    top_p=0.9,\n",
        "    temperature=0.2,\n",
        "    repeat_penalty=1.1\n",
        "):\n",
        "    model = Llama(\n",
        "        model_path=model_path,\n",
        "        n_ctx=n_ctx,\n",
        "        n_parts=1,\n",
        "        n_gpu_layers=-1,\n",
        "    )\n",
        "\n",
        "    system_tokens = get_system_tokens(model, context)\n",
        "    tokens = system_tokens\n",
        "    model.eval(tokens)\n",
        "\n",
        "    while True:\n",
        "        user_message = input(\"User: \")\n",
        "        message_tokens = get_message_tokens(model=model, role=\"user\", content=user_message)\n",
        "        role_tokens = [model.token_bos(), BOT_TOKEN, LINEBREAK_TOKEN]\n",
        "        tokens += message_tokens + role_tokens\n",
        "        generator = model.generate(\n",
        "            tokens,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            temp=temperature,\n",
        "            repeat_penalty=repeat_penalty\n",
        "        )\n",
        "        for token in generator:\n",
        "            token_str = model.detokenize([token]).decode(\"utf-8\", errors=\"ignore\")\n",
        "            tokens.append(token)\n",
        "            if token == model.token_eos():\n",
        "                break\n",
        "            print(token_str, end=\"\", flush=True)\n",
        "        print()\n",
        "        if user_message.lower().find(\"спасибо\") != -1:\n",
        "          break"
      ],
      "metadata": {
        "id": "tIaVp_2kerB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interact(MODEL_PATH, TEXT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSjSInYue5yA",
        "outputId": "a208a310-bb8e-449d-be2a-3b23d7bed123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Добрый день!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Здравствуйте! Как я могу помочь вам разобраться в корпоративной базе знаний?\n",
            "User: Расскажи, какие типы встреч у нас бывают\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "У нас есть несколько типов встреч: тактические (ТВ), законодательные (ЗВ) и целевые. Тактические встречи проводятся для решения текущих задач и обсуждения достижений, выполнения проектов и теншенов. Законодательные встречи используются для корректировки законодательства Круга. Целевые встречи проводятся для решения отдельных вопросов и могут включать в свой состав участников различных Кругов.\n",
            "User: Это все встречи или есть какие то еще?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Да, это все встречи. Если у вас есть другие вопросы, я готов помочь вам разобраться в них.\n",
            "User: А что ты знаешь про agile встречи?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agile встречи - это планирование спринта, груминг, демо спринта, дейли, System Demo и участие в PI планировании. Эти слова можно найти в глоссарии нашей базы знаний. Если у вас есть дополнительные вопросы по этому поводу, я готов помочь вам разобраться в них.\n",
            "User: Объясни простыми словами, что такое груминг\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Груминг - это процесс, когда команда собирается вместе для обсуждения задач и решения проблем. Это может происходить на физическом месте или в онлайн-конференции. Груминг помогает команде лучше понимать свои цели и задачи, а также работать более эффективно вместе.\n",
            "User: спасибо!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Пожалуйста! Если у вас есть еще вопросы, я готов помочь вам разобраться в них.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vLXK9Ymfw2cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LangChain"
      ],
      "metadata": {
        "id": "-N9xQiqGl6zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ],
      "metadata": {
        "id": "VjIHjUX8l9SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"User: {question}\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "1DFEurEqmAHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
      ],
      "metadata": {
        "id": "gXvBPtJVmJPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_gpu_layers = 15000  # Change this value based on your model and your GPU VRAM pool.\n",
        "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/model-q4_K.gguf\",\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True, # Verbose is required to pass to the callback manager\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMEwNjrdmPYb",
        "outputId": "ab045d51-16ab-4aea-bda7-7242018145de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "question = \"Нужно ли мыть посуду?\"\n",
        "llm_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "9PSTc7RGmSsx",
        "outputId": "4f9b54b1-2cd3-4fa8-c72e-99617d7bf752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " bot: Добрый день! Очень важна сама процедура мытья посуды, так как она помогает сохранить ее чистоту и продлить срок службы. Однако важно отметить, что не всегда нужно мыть посуду после каждого применения. Например, в большинстве случаев следует мыть пластиковую посуду только после использования или если она стала грязной вне зависимости от того, была ли ее использована. Некоторые виды посуды, такие как металлические и стеклянные приборы, можно мыть в любое время, но важно следить за тем, чтобы они не получили повреждений во время процесса чистки."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n bot: Добрый день! Очень важна сама процедура мытья посуды, так как она помогает сохранить ее чистоту и продлить срок службы. Однако важно отметить, что не всегда нужно мыть посуду после каждого применения. Например, в большинстве случаев следует мыть пластиковую посуду только после использования или если она стала грязной вне зависимости от того, была ли ее использована. Некоторые виды посуды, такие как металлические и стеклянные приборы, можно мыть в любое время, но важно следить за тем, чтобы они не получили повреждений во время процесса чистки.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ],
      "metadata": {
        "id": "OQPNQLecqeIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_template = \"\"\"\n",
        "Тебя зовут Сайга. Ты полезный, уважительный и честный ассистент. Для ответа на вопрос пользователя используй информацию из контекста.\n",
        "Если ты не знаешь ответа, просто скажи, что не знаешь, не пытайся придумать ответ.\n",
        "Контекст: {context}\n",
        "Вопрос: {question}\n",
        "Возвращай только полезный ответ и ничего больше.\n",
        "Полезный ответ:\n",
        "\"\"\"\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    model_path=MODEL_PATH,\n",
        "    n_ctx= 2400,\n",
        "    verbose=True,\n",
        "    use_mlock=True,\n",
        "    n_gpu_layers=12,\n",
        "    n_threads=4,\n",
        "    n_batch=512\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "combine_docs_custom_prompt = PromptTemplate.from_template(system_template)\n",
        "\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    combine_docs_chain_kwargs=dict(prompt=combine_docs_custom_prompt),\n",
        "    memory=memory,\n",
        ")"
      ],
      "metadata": {
        "id": "9LmIOHxcqFzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13B"
      ],
      "metadata": {
        "id": "gh-l_3QNVqHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/IlyaGusev/saiga2_13b_gguf/resolve/main/model-q2_K.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "E2uR8UahVsC_",
        "outputId": "3291128e-f527-47c4-f0d1-85b510aae82f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-29 19:07:49--  https://huggingface.co/IlyaGusev/saiga2_13b_gguf/resolve/main/model-q2_K.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.124, 18.172.134.88, 18.172.134.4, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/ab/94/ab947ab33d7e2e2456ff30395e2002ba0873cd3831ebb1746d8b54903f298d86/852b03a3a1b89c3ec3eba995819ac9c916eef54f430f6806e4cfb2fb3e690edd?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-q2_K.gguf%3B+filename%3D%22model-q2_K.gguf%22%3B&Expires=1696273669&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NjI3MzY2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9hYi85NC9hYjk0N2FiMzNkN2UyZTI0NTZmZjMwMzk1ZTIwMDJiYTA4NzNjZDM4MzFlYmIxNzQ2ZDhiNTQ5MDNmMjk4ZDg2Lzg1MmIwM2EzYTFiODljM2VjM2ViYTk5NTgxOWFjOWM5MTZlZWY1NGY0MzBmNjgwNmU0Y2ZiMmZiM2U2OTBlZGQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=kNrIdOzLyz6Zx%7E4fgi46uW%7Eq6ScpNSTCacxwMG05rFdqallu%7E-eejkJosGVycbte%7EZGdBgD2390CShNyoTrix9l7z2J57gJ3mg-SQuTpa%7EUm5WG7FHdaUVc9UWe7sI3nJkdJYofnEwwi95s76VDCgn3kx1QXXGZ5gJ8PdRW-JrTNHfRdM5GMampuRhMRZb3r3UeCr1QWNfI7%7E91N9UJG4pFPTPv2KUDQoUQhEQCaGqwE8rBYDhufk6uIfJfxbrdhsEJUI4UxRyUIJ%7ERgqPwJ2giGJPAF2rFCKfyfwUlxNc94t1aa4oe9h8ZVusk%7EriyZYmYQAnBsxYU8%7E2yGeikerg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-09-29 19:07:49--  https://cdn-lfs.huggingface.co/repos/ab/94/ab947ab33d7e2e2456ff30395e2002ba0873cd3831ebb1746d8b54903f298d86/852b03a3a1b89c3ec3eba995819ac9c916eef54f430f6806e4cfb2fb3e690edd?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-q2_K.gguf%3B+filename%3D%22model-q2_K.gguf%22%3B&Expires=1696273669&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NjI3MzY2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9hYi85NC9hYjk0N2FiMzNkN2UyZTI0NTZmZjMwMzk1ZTIwMDJiYTA4NzNjZDM4MzFlYmIxNzQ2ZDhiNTQ5MDNmMjk4ZDg2Lzg1MmIwM2EzYTFiODljM2VjM2ViYTk5NTgxOWFjOWM5MTZlZWY1NGY0MzBmNjgwNmU0Y2ZiMmZiM2U2OTBlZGQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=kNrIdOzLyz6Zx%7E4fgi46uW%7Eq6ScpNSTCacxwMG05rFdqallu%7E-eejkJosGVycbte%7EZGdBgD2390CShNyoTrix9l7z2J57gJ3mg-SQuTpa%7EUm5WG7FHdaUVc9UWe7sI3nJkdJYofnEwwi95s76VDCgn3kx1QXXGZ5gJ8PdRW-JrTNHfRdM5GMampuRhMRZb3r3UeCr1QWNfI7%7E91N9UJG4pFPTPv2KUDQoUQhEQCaGqwE8rBYDhufk6uIfJfxbrdhsEJUI4UxRyUIJ%7ERgqPwJ2giGJPAF2rFCKfyfwUlxNc94t1aa4oe9h8ZVusk%7EriyZYmYQAnBsxYU8%7E2yGeikerg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.185.27, 18.154.185.26, 18.154.185.94, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.185.27|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5429348096 (5.1G) [binary/octet-stream]\n",
            "Saving to: ‘model-q2_K.gguf’\n",
            "\n",
            "model-q2_K.gguf     100%[===================>]   5.06G  42.0MB/s    in 1m 57s  \n",
            "\n",
            "2023-09-29 19:09:46 (44.3 MB/s) - ‘model-q2_K.gguf’ saved [5429348096/5429348096]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"/content/model-q2_K.gguf\"\n",
        "SYSTEM_PROMPT = \"Ты - русскоязычная вопросно-ответная система, отвечающая на вопрос по тексту. Тебе на вход подается текст и вопрос. Проанализируй текст и дай ответ на вопрос. Используй только предоставленный текст.\""
      ],
      "metadata": {
        "id": "HVw3IvraWUkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(MODEL_PATH, SYSTEM_PROMPT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b7d9b37c-3795-4086-aba1-0441525fc919",
        "id": "0t8fSfIPWUkR"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "answer = agent.interact(\"Текст:\" + TEXT + \"Вопрос:\" + QUESTION)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAAUJSfzW-5v",
        "outputId": "3f3df1f1-664f-4aa2-9c3c-262b388b1b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 58 s, sys: 6.56 s, total: 1min 4s\n",
            "Wall time: 1min 5s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "HgM01NmpXMFK",
        "outputId": "d7006176-161b-4686-d6af-62be856426ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'- Наш офис\\n\\nУ нас есть несколько правил по нашим общим пространствам в офисе:\\n\\nНа кухне у нас есть три холодильника:\\n    - два из них стандартно предназначены для хранения обедов сотрудников (постарайся не перепутать свой контейнер с контейнером коллеги ☺️)(номерки на картинке 1 и 2)\\n    - третий (со стеклянной дверцей) (3 на картинке, наш любименький) – общий, т.е. всё, что находится в нём может использовать любой сотрудник) Но, как известно, в большой семье... Так что успевай:-))\\n    - А что у нас с посудой? Мыть или не мыть, вот в чем вопрос\\nПосле еды мы не моем самостоятельно посуду, а ставим посуду в посудомоечную машину (предварительно убрав остатки еды). На машине есть наклейка в зависимости от ее статуса работы (см.картинку):«Жду посуду» - открываете и ставите в нее посуду самостоятельно; если наклейка «Работаю» – необходимо поставить посуду в раковину, наши Феи позже с помощью магии \"вжуух\" и всё перенесут в машину. При этом при работе посудомоечной машины открывать ее запрещено.Офисные разделочные ножи и доски моем самостоятельно.\\n\\n- Правила кухни и офиса\\n  - Пролил что-то или просыпал - протри/подмети, пожалуйста (тряпка/веник и совок в сан.узле около кухни)\\n  - Увидел забытую грязную кружку/тарелку друга на кухонном столе - убери в посудомойку ;-)\\n  - Если брали вазу под цветы, после помой, пожалуйста, и протри ее))\\n  - Пожалуйста, пользуйся в офисе сменной обувью)\\n\\n- Чаты и каналы связи\\nВот здесь у нас размещены наши общие телеграмм-чаты, каналы связи, которые мы используем в работе и наши сервисы, которые будут тебе полезны в работе \\n- Телеграмм – здесь у нас проходит основное общение))\\nВот список чатов, который могут быть тебе полезны:\\n  - SmartConsulting – наш основной чат. Наши HR обязательно добавят тебя в него и представят команде, чтобы все ребята могли тебя поприветствовать;-)\\n  - SC News - здесь ты можешь узнать все новости компании\\n  - Legal News - новости об изменениях законодательства РФ и другие юридические тонкости'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Инструкция Гусева"
      ],
      "metadata": {
        "id": "x6tngBivDyrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/IlyaGusev/rulm/master/self_instruct/src/interact_llamacpp.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ_ZsI6uC57I",
        "outputId": "13f1cd18-28c7-4a67-e9b2-a956b999bdda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-29 17:47:16--  https://raw.githubusercontent.com/IlyaGusev/rulm/master/self_instruct/src/interact_llamacpp.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1951 (1.9K) [text/plain]\n",
            "Saving to: ‘interact_llamacpp.py’\n",
            "\n",
            "interact_llamacpp.p 100%[===================>]   1.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-09-29 17:47:17 (29.4 MB/s) - ‘interact_llamacpp.py’ saved [1951/1951]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fire\n",
        "from llama_cpp import Llama\n",
        "\n",
        "SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n",
        "SYSTEM_TOKEN = 1788\n",
        "USER_TOKEN = 1404\n",
        "BOT_TOKEN = 9225\n",
        "LINEBREAK_TOKEN = 13\n",
        "\n",
        "ROLE_TOKENS = {\n",
        "    \"user\": USER_TOKEN,\n",
        "    \"bot\": BOT_TOKEN,\n",
        "    \"system\": SYSTEM_TOKEN\n",
        "}\n",
        "\n",
        "\n",
        "def get_message_tokens(model, role, content):\n",
        "    message_tokens = model.tokenize(content.encode(\"utf-8\"))\n",
        "    message_tokens.insert(1, ROLE_TOKENS[role])\n",
        "    message_tokens.insert(2, LINEBREAK_TOKEN)\n",
        "    message_tokens.append(model.token_eos())\n",
        "    return message_tokens\n",
        "\n",
        "\n",
        "def get_system_tokens(model):\n",
        "    system_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": SYSTEM_PROMPT\n",
        "    }\n",
        "    return get_message_tokens(model, **system_message)\n",
        "\n",
        "\n",
        "def interact(\n",
        "    model_path,\n",
        "    n_ctx=2000,\n",
        "    top_k=30,\n",
        "    top_p=0.9,\n",
        "    temperature=0.2,\n",
        "    repeat_penalty=1.1\n",
        "):\n",
        "    model = Llama(\n",
        "        model_path=model_path,\n",
        "        n_ctx=n_ctx,\n",
        "        n_parts=1,\n",
        "    )\n",
        "\n",
        "    system_tokens = get_system_tokens(model)\n",
        "    tokens = system_tokens\n",
        "    model.eval(tokens)\n",
        "\n",
        "    while True:\n",
        "        user_message = input(\"User: \")\n",
        "        message_tokens = get_message_tokens(model=model, role=\"user\", content=user_message)\n",
        "        role_tokens = [model.token_bos(), BOT_TOKEN, LINEBREAK_TOKEN]\n",
        "        tokens += message_tokens + role_tokens\n",
        "        generator = model.generate(\n",
        "            tokens,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            temp=temperature,\n",
        "            repeat_penalty=repeat_penalty\n",
        "        )\n",
        "        for token in generator:\n",
        "            token_str = model.detokenize([token]).decode(\"utf-8\", errors=\"ignore\")\n",
        "            tokens.append(token)\n",
        "            if token == model.token_eos():\n",
        "                break\n",
        "            print(token_str, end=\"\", flush=True)\n",
        "        print()"
      ],
      "metadata": {
        "id": "Tpf5oGcIDSEk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}